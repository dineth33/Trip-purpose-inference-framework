{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00877560",
   "metadata": {},
   "source": [
    "# Combined Model and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a25cfbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\MSC\\\\Data'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.chdir('D:\\MSC\\Data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d416e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "from shapely.geometry import Point\n",
    "import math \n",
    "import shapely.speedups \n",
    "shapely.speedups.enable()\n",
    "from shapely.geometry import Polygon\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4d67f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML packages \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1c00b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d128edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800d88a",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde46212",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Data Import :</b> Import cleaned taxi, places, comtrans data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef66aec",
   "metadata": {},
   "source": [
    "### Places (POI) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84cba102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\GEO_NEW\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# prepare the places df before processsing\n",
    "# add the category/purpose \n",
    "places = pd.read_csv('places\\\\places for dss\\\\thi_jpura_dehi_cleaned_places_predicted_transformed.csv',index_col = 0)\n",
    "places = places.drop(places[places['type_1'] == 'gas_station'].index)\n",
    "places.reset_index(inplace = True, drop = True)\n",
    "places = places[['clean_name','lat','lng','type_1','no_of_ratings','avg_rating','purpose','Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46897955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# places gpd creation \n",
    "geometry = [Point(xy) for xy in zip(places.lng, places.lat)]\n",
    "placesgpd  = gpd.GeoDataFrame(places, crs=\"EPSG:4326\", geometry=geometry)\n",
    "placesgpd.crs = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64302580",
   "metadata": {},
   "source": [
    "### Taxi trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0f164b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips = pd.read_csv('pickme data\\\\nov\\\\thi_jupra_dehi_data_nov_45.csv', index_col = 0, parse_dates = [11,12,13])\n",
    "taxitrips = taxitrips[['trip_id', 'passenger_id', 'pickup_lat', 'pickup_long', 'dropoff_lat', 'dropoff_long', 'actual_pickup_time','pickup_time','drop_time']]\n",
    "\n",
    "# optimize the data types \n",
    "taxitrips['trip_id'] = taxitrips ['trip_id'].astype(np.int32)\n",
    "taxitrips[['pickup_lat', 'pickup_long', 'dropoff_lat', 'dropoff_long']] = taxitrips[['pickup_lat', 'pickup_long', 'dropoff_lat', 'dropoff_long']].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7053b8",
   "metadata": {},
   "source": [
    "### Route network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8170fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and filtering out the roads within the colombo district \n",
    "slroads = gpd.read_file('road network\\hotosm_lka_roads_lines.shp',crs = {'init': 'epsg:4326'})\n",
    "sri_lanka = gpd.read_file('administration shapes\\lka_admbnda_adm2_slsd_20200305.shp',crs = {'init': 'epsg:4326'})\n",
    "colombo = sri_lanka[sri_lanka['ADM2_EN']=='Colombo']\n",
    "colombo.reset_index(drop=True, inplace=True)\n",
    "mask = slroads.within(colombo.loc[0,'geometry'])\n",
    "colomboroads = slroads.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0938a",
   "metadata": {},
   "source": [
    "### Inputs for baysean model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "127f4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the baysean inference \n",
    "# huanggrid \n",
    "huanggrid = pd.read_csv('inputs\\HuangGrid.csv',index_col = 0, parse_dates = [0])\n",
    "huanggrid.columns = huanggrid.columns.str.replace(' ', '')\n",
    "huanggrid['Day'] = huanggrid['Day'].str.replace(' ', '')\n",
    "\n",
    "# time df \n",
    "time_df = pd.read_csv('places\\categories\\category_type_times.csv',index_col=0)\n",
    "time_df.columns = time_df.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e303a20",
   "metadata": {},
   "source": [
    "### Common functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939b451",
   "metadata": {},
   "source": [
    "#### staight line distance calculation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8d5a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, sqrt ## source: STACKOVERFLOW\n",
    "\n",
    "def qick_distance(Lat1, Long1, Lat2, Long2):\n",
    "\n",
    "    \"\"\" \n",
    "    calculates the harvesine distance between two cordinates \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    math\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    Lat1, Long1 - cordinates of point 1 \n",
    "    Lat2, Long2 -  cordinates of point 2 \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    harvesine distance between the two points  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = Lat2 - Lat1\n",
    "    y = (Long2 - Long1) * cos((Lat2 + Lat1)*0.00872664626)  \n",
    "    \n",
    "    return 111.319 * sqrt(x*x + y*y)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630153d",
   "metadata": {},
   "source": [
    "#### candidate pois selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545980f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_poi_selection(placesgpd, lat, long, drop_time, time_df, walking_radius = 100):\n",
    "        \n",
    "    \"\"\" \n",
    "    select the candidate POIs for a given timestamped taxi drop off point \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    geopandas \n",
    "    geopandas dataframe of the places (POIs), (placesgpd)\n",
    "    dataframe of places opening times (time_df)\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat, long -  cordinates of the drop off point (DOP)\n",
    "    drop_time -  timestamp of the DOP\n",
    "    walking_radius - maximum allowable radius for a person to walk once got off from the taxi (defalut - 100m)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    geopandas df of the selected candidate POIs\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    Point_DOP = Point(long,lat) \n",
    "    Buffer = Point_DOP.buffer(walking_radius*0.001/110) ## (110m = 0.001) \n",
    "    mask = placesgpd.within(Buffer) ## filter the data for the required buffer zone \n",
    "    \n",
    "    selected_places = placesgpd.loc[mask] ## POIs within the radius for the DOP \n",
    "    \n",
    "    DOP_time = float(str(drop_time.hour) + '.' + str(drop_time.minute))\n",
    "    DOP_day = drop_time.day_of_week\n",
    "    \n",
    "    for row in selected_places.itertuples():\n",
    "\n",
    "        # for saturdays \n",
    "        if DOP_day == 5:\n",
    "            \n",
    "            # take the opening time by the trip purpose of a POI for the unlabelled POIs. \n",
    "            if row.type_1 == 'establishment':\n",
    "                open_time = time_df.loc[row.purpose,'Open_Time_Sat']    \n",
    "                close_time = time_df.loc[row.purpose ,'Close_Time_Sat']\n",
    "                if DOP_time <= open_time or DOP_time >= close_time:\n",
    "                    selected_places = selected_places.drop(row.Index)  # drop the place if it is not within the defined opening time period \n",
    "                    \n",
    "            # take the opening time by the category for the labelled POIs. \n",
    "            else:\n",
    "                open_time = time_df.loc[row.type_1,'Open_Time_Sat']    \n",
    "                close_time = time_df.loc[row.type_1 ,'Close_Time_Sat']\n",
    "                if DOP_time <= open_time or DOP_time >= close_time:\n",
    "                    selected_places = selected_places.drop(row.Index)  \n",
    "\n",
    "        # for sundays \n",
    "        elif DOP_day == 6:\n",
    "            if row.type_1 == 'establishment':\n",
    "                open_time = time_df.loc[row.purpose,'Open_Time_Sun']    \n",
    "                close_time = time_df.loc[row.purpose,'Close_Time_Sun']\n",
    "                if DOP_time <= open_time or DOP_time >= close_time:\n",
    "                    selected_places = selected_places.drop(row.Index) \n",
    "            else:\n",
    "                open_time = time_df.loc[row.type_1,'Open_Time_Sun']    \n",
    "                close_time = time_df.loc[row.type_1,'Close_Time_Sun']\n",
    "                if DOP_time <= open_time or DOP_time >= close_time:\n",
    "                    selected_places = selected_places.drop(row.Index) \n",
    "         \n",
    "        # for weekdays\n",
    "        else:\n",
    "            if row.type_1 == 'establishment':\n",
    "                open_time = time_df.loc[row.purpose,'Open_Time']    \n",
    "                close_time = time_df.loc[row.purpose,'Close_Time']\n",
    "                if DOP_time <= open_time or DOP_time >= close_time:\n",
    "                    selected_places = selected_places.drop(row.Index)  \n",
    "            else:\n",
    "                open_time = time_df.loc[row.type_1,'Open_Time']    \n",
    "                close_time = time_df.loc[row.type_1,'Close_Time']\n",
    "                if DOP_time <= open_time or DOP_time >= close_time:\n",
    "                    selected_places = selected_places.drop(row.Index)  \n",
    "                   \n",
    "    return selected_places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1385f",
   "metadata": {},
   "source": [
    "#### bayesian inference models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7152c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baysean_inference(candidate_pois, lat, long, drop_time, huanggrid):\n",
    "    \n",
    "    \"\"\" \n",
    "    inferring the purpose of a trip given the candidate pois, timestamped DOP and temporal impact for the purposes \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    pandas \n",
    "    candidate POI df \n",
    "    hunaggrid df \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat, long -  cordinates of the drop off point (DOP)\n",
    "    drop_time -  timestamp of the DOP\n",
    "    candidate_pois - df for the selected candidate POIs for the DOP\n",
    "    huanggrid - grid for the temporal impact for purposes. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    inferred trip purpose for the trip as a string \n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    gravity_values = {'work':8 ,'shopping':6 ,'personal':7 ,'dining':2 ,'medical':4 ,'education':5 , 'transit':3 , 'home':2, 'recreational':1}\n",
    "\n",
    "    day = drop_time.day_of_week # select the required part of the HuangGrid for the DOP day \n",
    "\n",
    "    if day == 6:\n",
    "           HuangGrid = huanggrid[huanggrid['Day']== 'sun'] \n",
    "    elif day == 5:\n",
    "           HuangGrid = huanggrid[huanggrid['Day']== 'sat']    \n",
    "    else:\n",
    "           HuangGrid = huanggrid[huanggrid['Day']== 'week']\n",
    "\n",
    "    # array to add the numerators of the baysean_probabilities    \n",
    "    baysean_numerators = np.array([])\n",
    "\n",
    "    candidate_pois.reset_index(inplace=True)\n",
    "    \n",
    "    for row in candidate_pois.itertuples():\n",
    "\n",
    "        # take huangvalue\n",
    "        Huangvalue = HuangGrid[HuangGrid.index.hour == drop_time.hour][row.purpose].values[0]\n",
    "        # take distance\n",
    "        distance = qick_distance(lat, long, row.lat, row.lng)\n",
    "        # calculate the numerators of the baysean model.\n",
    "        baysean_numerators = np.append(baysean_numerators, (gravity_values[row.purpose]/(distance)**1.5)*(Huangvalue))\n",
    "\n",
    "\n",
    "    # calcualte the denominator \n",
    "    baysean_sum  = baysean_numerators.sum()\n",
    "\n",
    "    # assign NA if there are no results \n",
    "    if baysean_sum == 0:\n",
    "            trip_purpose = 'NA'\n",
    "    \n",
    "    else:\n",
    "        # calcualte the bayesian probabilities \n",
    "        baysean_probabilities = baysean_numerators/baysean_sum\n",
    "\n",
    "        # select the one with the highest probability \n",
    "        index_for_maximum = np.where(baysean_probabilities == baysean_probabilities.max())[0]\n",
    "        \n",
    "        #check the multiple purpose assignment \n",
    "        if index_for_maximum.shape[0] == 1:\n",
    "            trip_purpose = candidate_pois.loc[index_for_maximum[0] , 'purpose']    \n",
    "        else:\n",
    "            trip_purpose = 'multiple'\n",
    "\n",
    "    return trip_purpose    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "086a61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baysean_inference_ln(candidate_pois, lat, long, drop_time, huanggrid):\n",
    "    \n",
    "    \"\"\" \n",
    "    inferring the purpose of a trip given the candidate pois, timestamped DOP and temporal impact for the purposes\n",
    "    based on the bayesian model that uses pois ratings for the attractiveness factor \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    pandas \n",
    "    candidate POI df \n",
    "    hunaggrid df \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat, long -  cordinates of the drop off point (DOP)\n",
    "    drop_time -  timestamp of the DOP\n",
    "    candidate_pois - df for the selected candidate POIs for the DOP\n",
    "    huanggrid - grid for the temporal impact for purposes. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    inferred trip purpose for the trip as a string \n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    day = drop_time.day_of_week # select the required part of the HuangGrid for the DOP day \n",
    "    \n",
    "    if day == 6:\n",
    "           HuangGrid = huanggrid[huanggrid['Day']== 'sun'] \n",
    "    elif day == 5:\n",
    "           HuangGrid = huanggrid[huanggrid['Day']== 'sat']    \n",
    "    else:\n",
    "           HuangGrid = huanggrid[huanggrid['Day']== 'week']\n",
    "    \n",
    "    candidate_pois['no_of_ratings'].fillna(0, inplace = True)\n",
    "    \n",
    "    # calculate the numerators of the bayesian probabilities    \n",
    "    for row in candidate_pois.itertuples():\n",
    "        \n",
    "        gravity_value = math.log(row.no_of_ratings + math.e) ## take the log e for the no of ratings as the gravity value \n",
    "        Huangvalue = HuangGrid[HuangGrid.index.hour == drop_time.hour][row.purpose].values[0]\n",
    "        distance = qick_distance(lat, long, row.lat, row.lng) \n",
    "        candidate_pois.loc[row.Index,'bayes_upper'] = (gravity_value/(distance)**1.5)*(Huangvalue)\n",
    "    \n",
    "    # purpose assignment \n",
    "\n",
    "    # assignment of \"NA\" for all the purpose expecting to change in future \n",
    "    trip_purpose = 'NA'\n",
    "    \n",
    "    # inferring the purpose based on the bayesian method \n",
    "    for row in candidate_pois.itertuples():\n",
    "        bayes_down = candidate_pois['bayes_upper'].sum()\n",
    "\n",
    "        if bayes_down != 0:\n",
    "            candidate_pois['bayes2015'] =  candidate_pois['bayes_upper']/bayes_down\n",
    "            maxvalue = candidate_pois['bayes2015'].max()\n",
    "            \n",
    "            #check the multiple purpose assignment \n",
    "            if (candidate_pois['bayes2015'] == maxvalue).sum() > 1: \n",
    "                trip_purpose = 'multiple' \n",
    "            else:\n",
    "                trip_purpose = candidate_pois[candidate_pois['bayes2015'] == maxvalue]['purpose']      \n",
    "        \n",
    "        else:\n",
    "            trip_purpose = 'NA'\n",
    "            \n",
    "    return trip_purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e470d5d",
   "metadata": {},
   "source": [
    "## Layer 1 : Regular trip purpose identification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a46714",
   "metadata": {},
   "source": [
    "At first, it is required to identify the passengers with regular trips. We only consider about the regularity within the weekdays. \n",
    "\n",
    "Required minimum number of days for trips is used as a variable to define regular passengers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f19a08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out weekday trips \n",
    "taxitrips['day'] = taxitrips['drop_time'].dt.day_of_week\n",
    "taxitrips_weekday = taxitrips[taxitrips['day'].isin([0,1,2,3,4])]\n",
    "\n",
    "# select the passengers with trips more than the defined number. \n",
    "regular_passengers = np.array(taxitrips_weekday.groupby('passenger_id').filter(lambda group: group['day'].count() > 1)['passenger_id'])\n",
    "regular_passengers = np.unique(regular_passengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670da63",
   "metadata": {},
   "source": [
    "There could be passengers with substantially higher number of trips which might be an error of the passenger_id attribute itself. Hence, it is required to remove such passengers from the identified regular passengers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1dfbce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any abnormal passengers \n",
    "abnormal = []\n",
    "for passenger in regular_passengers:\n",
    "    if taxitrips[taxitrips['passenger_id'] == passenger].shape[0] > 100:\n",
    "        abnormal.append(passenger)\n",
    "        \n",
    "regular_passengers = np.delete(regular_passengers, np.where(regular_passengers == abnormal[0])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c24255d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16050,)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_passengers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad9d93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> BEWARE: </b> Required functions for the purpose inference \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0bb57",
   "metadata": {},
   "source": [
    "### functions for regular trips "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44497d",
   "metadata": {},
   "source": [
    "#### clustering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c38be50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(pickup_data, dropoff_data):\n",
    "        \n",
    "    \"\"\" \n",
    "    cluster the origin and destinations of regular users seperately and take the assigned cluster numbers \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    numpy \n",
    "    pandas \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pickup_data -  df with positional data for origin \n",
    "    dropoff_data -  df with positional data the destination \n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    cluster numbers for origin and destination as numpy arrays \n",
    "    \n",
    "    \"\"\"  \n",
    "    \n",
    "    # clustering for origin \n",
    "    X1 =  pickup_data\n",
    "    model1 = DBSCAN(eps=0.0005, min_samples = 2) # minimum 3 trips and minimum 50m as parameters. \n",
    "    model1.fit(X1)\n",
    "    origin_cluster_labels = model1.labels_\n",
    "    \n",
    "    # clustering for destination \n",
    "    X2 =  dropoff_data\n",
    "    model2 = DBSCAN(eps=0.0005, min_samples = 2)\n",
    "    model2.fit(X2)\n",
    "    destination_cluster_labels = model2.labels_\n",
    "    \n",
    "    return origin_cluster_labels, destination_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "74575f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_labels, destination_labels = dbscan_clustering(passenger_data[['pickup_lat', 'pickup_long']], passenger_data[['dropoff_lat', 'dropoff_long']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e12b9",
   "metadata": {},
   "source": [
    "#### cluster compatability check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1fd36052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_pair_check(origin_cluster_labels, destination_cluster_labels, minimum_match):\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    evaulating the compatability of clustered origin and destination locations\n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    numpy \n",
    "    Counter \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    origin_cluster_labels -  cluster labels at the origin/pup from the function \"dbscan clustering\"\n",
    "    destination_cluster_labels -  cluster labels at the dop from the function \"dbscan clustering\"\n",
    "    minimum_match - required trips to consider as a regular trip\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary of matching indices for the cluster numbers\n",
    "    \n",
    "    \"\"\"      \n",
    " \n",
    "    matching_indexes = dict()\n",
    "\n",
    "    for no in np.unique(origin_cluster_labels):  # start with origin cluster numbers \n",
    "        if no == -1: # ignore the outliers \n",
    "            continue   \n",
    "        else:\n",
    "            origin_indexes = np.where(origin_cluster_labels == no)[0]\n",
    "\n",
    "            dop_cluster_count =  Counter(destination_cluster_labels[np.where(origin_cluster_labels == no)]) # count of cluster numbers at destination cluster numbers parallel to select3ed origin cluster number \n",
    "            \n",
    "            if -1 in dop_cluster_count: del dop_cluster_count[-1]\n",
    "                \n",
    "            matching_clusters = [k for k in dop_cluster_count if dop_cluster_count[k] >= minimum_match] # filter the numbers with required minimum matching \n",
    "\n",
    "        if len(matching_clusters) == 0: # ignore if there arent any matches \n",
    "                continue \n",
    "\n",
    "        for idx,value in enumerate(matching_clusters): # loop through matching cluster numbers (avoid possibility of having two destination clusters for one origin cluster)\n",
    "            destination_indexes = np.where(destination_cluster_labels == value)[0]\n",
    "            matched_indexes = np.intersect1d(origin_indexes, destination_indexes)\n",
    "            matching_indexes[value] = matched_indexes\n",
    "            \n",
    "#           matching_indexes.append(matched_indexes) # append the array of matched indices \n",
    "\n",
    "    return matching_indexes \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1d04f",
   "metadata": {},
   "source": [
    "#### time bin compatability check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ca7c8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_check(passenger_data, destination_cluster_labels, minium_match,  **time_range):\n",
    "        \n",
    "    \"\"\" \n",
    "    evaluate the compatibility between the defined time bins for regular trip purposes (work and education) and drop off times\n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    numpy \n",
    "    Counter \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    passenger_data -  trip df for the selected passenger \n",
    "    destination_cluster_labels -  cluster labels at the dop from the function \"dbscan clustering\"\n",
    "    minimum_match - required trips to consider as a regular trip\n",
    "    **time_range - {work1: 1st time period for work, work2: 2nd time period for work, education: time period for education}\n",
    "    note: time ranges should be add argument names as 'work1', 'work2', 'education, as lists like [6,10]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary of matching destination cluster number and the assigned time comptability sign \n",
    "    return an array for the compatibility with time bins as 0 - non,  1 - work, 2, - education, 3 - work + education\n",
    "    \n",
    "    if shape of the output array = 0 - no compatibility, >1 - complex compatibility, 1 - acceptable\n",
    "    \n",
    "    \"\"\"      \n",
    "     \n",
    "    regular_purpose = int() # 0 - non,  1 - work, 2 - work + education\n",
    "    regular_purposes = dict() # assuming if two dop clusters meet the time frame requirements, it will be saved in the dictionary    \n",
    "    \n",
    "    for no in np.unique(destination_cluster_labels):\n",
    "        if no == -1: # ignore the outliers \n",
    "            continue   \n",
    "        else:\n",
    "            cluster_hours = passenger_data.loc[np.where(destination_cluster_labels == no)]['drop_time'].dt.hour.values # take the hours for the selected dop cluster \n",
    "            \n",
    "            work1_match = ((cluster_hours >= time_range['work1'][0]) & (cluster_hours <= time_range['work1'][1])).sum()\n",
    "            work2_match = ((cluster_hours >= time_range['work2'][0]) & (cluster_hours <= time_range['work2'][1])).sum()\n",
    "            education_match = ((cluster_hours >= time_range['education'][0]) & (cluster_hours <= time_range['education'][1])).sum()\n",
    "            \n",
    "            if work1_match >= minium_match:\n",
    "                regular_purpose += 1   # add the defined values for possible regular purposes by passenger\n",
    "            \n",
    "            elif work2_match >= minium_match:\n",
    "                regular_purpose += 1\n",
    "            \n",
    "            elif work1_match + work2_match >= minium_match:\n",
    "                regular_purpose += 1            \n",
    "            \n",
    "            if education_match >=  minium_match:\n",
    "                regular_purpose += 2          \n",
    "\n",
    "            regular_purposes[no] = regular_purpose\n",
    "\n",
    "    return regular_purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56df5f5",
   "metadata": {},
   "source": [
    "#### return trip check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "69d4b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_trip_check (onward_indices, passenger_data, matched_indices):\n",
    "    \n",
    "    \"\"\" \n",
    "    identify the existence of round regular trips and if so, the indices of those\n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    numpy \n",
    "    qick_distance function \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    onward_indices - \n",
    "    passenger_data -  trip df for the selected passenger \n",
    "    matched_indices - output of the od_pair_check function \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    boolean for return trip existence, arrays of indices for return trips\n",
    "    \n",
    "    \"\"\"       \n",
    "\n",
    "    round_trip = False \n",
    "    return_indices = None \n",
    "    \n",
    "    onwards_drop_lat, onwards_drop_long = passenger_data.loc[onward_indices,'dropoff_lat'].mean(), passenger_data.loc[onward_indices,'dropoff_long'].mean()\n",
    "    onwards_pick_lat, onwards_pick_long = passenger_data.loc[onward_indices,'pickup_lat'].mean(), passenger_data.loc[onward_indices,'pickup_long'].mean()\n",
    "        \n",
    "    for key in matched_indices:\n",
    "        \n",
    "        return_drop_lat, return_drop_long = passenger_data.loc[matched_indices[key],'dropoff_lat'].mean(), passenger_data.loc[matched_indices[key],'dropoff_long'].mean()\n",
    "        return_pick_lat, return_pick_long = passenger_data.loc[matched_indices[key],'pickup_lat'].mean(), passenger_data.loc[matched_indices[key],'pickup_long'].mean()\n",
    "   \n",
    "        if (qick_distance(onwards_drop_lat, onwards_drop_long, return_pick_lat, return_pick_long) <= 50) & (qick_distance(onwards_pick_lat, onwards_pick_long, return_drop_lat, return_drop_long) <= 50) :\n",
    "            \n",
    "            round_trip = True\n",
    "            return_indices = matched_indices[key]\n",
    "            \n",
    "    return round_trip, return_indices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349bd6aa",
   "metadata": {},
   "source": [
    "#### round_trip_purpose_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a9cfe48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_trip_purpose_inference(candidate_pois, onward_indices, return_indices,  time_bin_purpose, passenger_data):\n",
    "    \n",
    "    \"\"\" \n",
    "    trip purpose inference based on the regular trips for round trips\n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    numpy \n",
    "    qick_distance function \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    onward_indices - indices of a clustered and time matches set of regular trips\n",
    "    passenger_data -  trip df for the selected passenger \n",
    "    candidate_pois - selected POIs around the dop \n",
    "    return_indices - output of function return trip check \n",
    "    time_bin_purpose - assigned purpose from the matched time bin {1 - work, 2 - educational, 3 - work + education}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trip purpose imputed passenger_data df : {0 - home, 1 - work, 2 - educational}\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if time_bin_purpose == 1: # work\n",
    "        passenger_data.loc[onward_indices,'Trip purpose'] = 1 \n",
    "        passenger_data.loc[return_indices,'Trip purpose'] = 0 \n",
    "    \n",
    "    elif time_bin_purpose == 2:\n",
    "        passenger_data.loc[onward_indices,'Trip purpose'] = 2 \n",
    "        passenger_data.loc[return_indices,'Trip purpose'] = 0 \n",
    "    \n",
    "    elif time_bin_purpose == 3:\n",
    "        \n",
    "        # check any educational POI exists \n",
    "        if 'education' in  candidate_pois['purpose'].values:\n",
    "            # bayes inference. \n",
    "            trip_purpose  = baysean_inference(candidate_pois, latitude, longitude, drop_time, huanggrid)\n",
    "        \n",
    "            if trip_purpose == 'education':\n",
    "                passenger_data.loc[onward_indices,'Trip purpose'] = 2\n",
    "                passenger_data.loc[return_indices,'Trip purpose'] = 0    \n",
    "                \n",
    "            else:\n",
    "                passenger_data.loc[onward_indices,'Trip purpose'] = 1 \n",
    "                passenger_data.loc[return_indices,'Trip purpose'] = 0 \n",
    "                \n",
    "        else:\n",
    "            passenger_data.loc[onward_indices,'Trip purpose'] = 1 \n",
    "            passenger_data.loc[return_indices,'Trip purpose'] = 0 \n",
    "\n",
    "          \n",
    "    return passenger_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce99bf",
   "metadata": {},
   "source": [
    "#### onward_trip_purpose_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8170110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onward_trip_purpose_inference(candidate_pois, onward_indices, time_bin_purpose, passenger_data):\n",
    "    \n",
    "    \"\"\" \n",
    "    trip purpose inference based on the regular trips for onward trips\n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    numpy \n",
    "    qick_distance function \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    onward_indices - indices of a clustered and time matches set of regular trips\n",
    "    passenger_data -  trip df for the selected passenger \n",
    "    candidate_pois - selected POIs around the dop \n",
    "    time_bin_purpose - {1 - work, 2 - educational, 3 - work + education}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trip purpose imputed passenger_data df : {1 - work, 2 - educational}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if time_bin_purpose == 1: # work\n",
    "        passenger_data.loc[onward_indices,'Trip purpose'] = 1 \n",
    "      \n",
    "    elif time_bin_purpose == 2:\n",
    "        passenger_data.loc[onward_indices,'Trip purpose'] = 2 \n",
    "        \n",
    "    elif time_bin_purpose == 3:\n",
    "       \n",
    "        # check any educational POI exists \n",
    "        if 'education' in  candidate_pois['purpose'].values:\n",
    "            # bayes inference. \n",
    "            trip_purpose  = baysean_inference(candidate_pois, latitude, longitude, drop_time, huanggrid)\n",
    "\n",
    "            if trip_purpose == 'education':\n",
    "                passenger_data.loc[onward_indices,'Trip purpose'] = 2 \n",
    "            else:\n",
    "                passenger_data.loc[onward_indices,'Trip purpose'] = 1 \n",
    "        else:\n",
    "            passenger_data.loc[onward_indices,'Trip purpose'] = 1 \n",
    "               \n",
    "    return passenger_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b437b",
   "metadata": {},
   "source": [
    "### Inferring the regular trip purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7291fe73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "purpose_identified_regular_trips = pd.DataFrame(columns = taxitrips.columns)\n",
    "passengers_without_clusters = []\n",
    "\n",
    "for passenger in regular_passengers:\n",
    "    \n",
    "    passenger_data = taxitrips[taxitrips['passenger_id'] == passenger].reset_index() # df reseted indices will be used throughout the analysis\n",
    "    \n",
    "    origin_labels, destination_labels = dbscan_clustering(passenger_data[['pickup_lat', 'pickup_long']], passenger_data[['dropoff_lat', 'dropoff_long']] )\n",
    "    \n",
    "    # terminate if there are no clustes (all the labels are -1)\n",
    "    if destination_labels.sum()/destination_labels.shape[0] == -1:\n",
    "        continue \n",
    "    \n",
    "    # time bin compatibility assessment  \n",
    "    regular_purposes = time_check(passenger_data, destination_labels, minium_match = 2, work1 = [6,10], work2 = [12,14], education = [6,9])\n",
    "    regular_purposes_keys = list(regular_purposes.keys()) \n",
    "    regular_purposes_values = list(regular_purposes.values())      \n",
    "    \n",
    "    time_matched_clusters = [x for x in regular_purposes_values if x>=1] \n",
    "\n",
    "    # terminate the processessing if 1.) no clusters (0), 2. time bins doesnt match(), 3. user has two different locations(>1) for a single time bin   \n",
    "    if len(time_matched_clusters) != 1: \n",
    "            continue \n",
    "    else:\n",
    "        time_bin_purpose = time_matched_clusters[0]\n",
    "                     \n",
    "    # od pair compatibility assessment \n",
    "    matched_indices = od_pair_check(origin_labels, destination_labels, minimum_match = 2)\n",
    "    if len(matched_indices) == 0:\n",
    "            continue \n",
    "            \n",
    "    # check whether the time bined cluster has a matched od pair\n",
    "    matched_onward_cluster = regular_purposes_keys[regular_purposes_values.index(time_matched_clusters[0])]\n",
    "    \n",
    "    if matched_onward_cluster in list(matched_indices.keys()):\n",
    "        onward_indices = matched_indices[regular_purposes_keys[regular_purposes_values.index(time_matched_clusters[0])]]\n",
    "    \n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    # check for the minimum regularity again \n",
    "    if len(onward_indices) < 2:\n",
    "        continue \n",
    "        \n",
    "    # variable selection for candiate poi selection \n",
    "    latitude = passenger_data.loc[onward_indices,'dropoff_lat'].mean()\n",
    "    longitude = passenger_data.loc[onward_indices, 'dropoff_long'].mean()\n",
    "    drop_time =  passenger_data.loc[onward_indices[0], 'drop_time'] # takes only one as its already on the bins \n",
    "        \n",
    "    # candidate poi selection    \n",
    "    candidate_pois = candidate_poi_selection(placesgpd, lat, long, drop_time, time_df, walking_radius = 100)\n",
    "                \n",
    "    # purpose assignment    \n",
    "    # check for round trips \n",
    "    if len(matched_indices) >= 2:\n",
    "        round_trip, return_indices  = return_trip_check(onward_indices, passenger_data, matched_indices)\n",
    "    \n",
    "        #   Scenario 1 (Round trip)\n",
    "        if round_trip == True:\n",
    "            passenger_data = round_trip_purpose_inference(candidate_pois, onward_indices, return_indices,  time_bin_purpose, passenger_data)\n",
    "            \n",
    "        # Scenario 2 (onward trip)\n",
    "        else:\n",
    "            passenger_data = onward_trip_purpose_inference(candidate_pois, onward_indices, time_bin_purpose, passenger_data)\n",
    "         \n",
    "    # Scenario 2 (onward trip)\n",
    "    else:\n",
    "        passenger_data = onward_trip_purpose_inference(candidate_pois, onward_indices, time_bin_purpose, passenger_data)\n",
    "        \n",
    "    passenger_data = passenger_data.set_index('index') # required with the appending \n",
    "    \n",
    "    purpose_identified_regular_trips = purpose_identified_regular_trips.append(passenger_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "066703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_identified_regular_trips = purpose_identified_regular_trips.dropna(subset=['Trip purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ab7fb70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7145, 10)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purpose_identified_regular_trips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a896b335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    6032\n",
       "2.0     945\n",
       "0.0     168\n",
       "Name: Trip purpose, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purpose_identified_regular_trips['Trip purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bd870fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_identified_regular_trips.to_csv('results\\\\threelayer results\\\\regular_2days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe9223",
   "metadata": {},
   "source": [
    "## Layer 2: Identifying trips attracted to residential places "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "8568dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_after_regular_inference = taxitrips.loc[taxitrips.index.difference(purpose_identified_regular_trips.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "d2e7c6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100472, 10)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxitrips_after_regular_inference.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351eb59",
   "metadata": {},
   "source": [
    "- Houses are not included in the POI dataset. Neglecting this assign incorrect purpose for trip destined for houses.\n",
    "\n",
    "- Developed method is based on three assumptions!  <br/>\n",
    "    1. DOPs per day can’t be located as groups (clusters)/ daily <br/>\n",
    "    2. DOPs  shouldn’t locate with a very close proximity to the main road  <br/>\n",
    "    3. If the trips attracted away from main roads belongs to a POI, it should be within a very narrow distance i.e. 15m!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfdf355",
   "metadata": {},
   "source": [
    "### 1. Filtering the grouped DOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76259ae",
   "metadata": {},
   "source": [
    "according to the first assumption, the DBSCAN algorithm is used to identify the groups of taxi dops. then those groups are removed and the only the outliers are kept.\n",
    "\n",
    "Note : this analysis is done based on data for a single day. because when dealing with data for a period of time, definetly there could be groups when the passengers are using the service to fulfill the mandatory activities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1704b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustered_trip_removal(taxitrips_after_regular_inference, allowable_dops = 2):\n",
    "        \n",
    "    \"\"\" \n",
    "    remove the grouped dops on daily basis \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    DBSCAN\n",
    "    pandas\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    taxitrips_after_regular_inference - filtered trips after purpose imputation based on regularity \n",
    "    allowable_dops -  maximum allowable dops for a cluster to keep \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trips without grouped dops on daily basis  \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # getting the dates and creating a group object to process the analysis day by day \n",
    "    dates = taxitrips_after_regular_inference['drop_time'].dt.day.unique()\n",
    "    taxidata_perday_g = taxitrips_after_regular_inference.groupby(taxitrips_after_regular_inference['drop_time'].dt.day)\n",
    "    \n",
    "    possible_home = pd.DataFrame(columns = taxitrips_after_regular_inference.columns)\n",
    "    for day in dates:\n",
    "        taxidata_perday = taxidata_perday_g.get_group(day)\n",
    "        X = taxidata_perday[['dropoff_lat','dropoff_long']]\n",
    "        model = DBSCAN(eps=0.0002, min_samples = allowable_dops) #20m for the eps\n",
    "        model.fit(X)\n",
    "        clusterlabels = model.labels_\n",
    "        taxidata_perday['cluster_no'] = clusterlabels.tolist()\n",
    "        selected_data = taxidata_perday[taxidata_perday['cluster_no']==-1]\n",
    "        possible_home = possible_home.append(selected_data)\n",
    "    \n",
    "    return possible_home \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3c452",
   "metadata": {},
   "source": [
    "### 2. Filtering DOPs closer to main roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6116f24",
   "metadata": {},
   "source": [
    "Before going to removal of drop off points near to the roads, we have to go calculate the distance that each drop off point has from the nearest main road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4520c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to measure the minimum distance to road from DOPs\n",
    "\n",
    "def min_road_distance_measure(possible_home,colombo_roads):\n",
    "    \n",
    "    \"\"\" \n",
    "    measure the distance for dops from the nearest main road \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    shapely \n",
    "    geopandas \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    possible_home - filtered trips after purpose imputation based on regularity \n",
    "    colombo_roads - shapefile of main roads for the selected area\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    minimum distance imputed possible_home df \n",
    "        \n",
    "    \"\"\"    \n",
    "    \n",
    "    for row in possible_home.itertuples():\n",
    "        \n",
    "        Point_DOP =  Point(row.dropoff_long, row.dropoff_lat)\n",
    "        distancelist = np.array([])\n",
    "        \n",
    "        for rows in colombo_roads.itertuples():\n",
    "            \n",
    "            distance = (Point_DOP.distance(rows.geometry))*10000\n",
    "            \n",
    "            distancelist = np.append(distancelist, distance)\n",
    "        \n",
    "        possible_home.loc[row.Index,'road_min_distace'] = np.min(distancelist)\n",
    "        \n",
    "    return possible_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6106b3a",
   "metadata": {},
   "source": [
    "with that we can go for the clustering, just like before the number of is determined not only by referring the elbow, but also considering the maximum of the distances  get from the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c9a46323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the clusters closer to the main road\n",
    "\n",
    "def distance_based_trip_removal(possible_home, clusters = 10, n_clusters_toremove = 2):\n",
    "  \n",
    "    \"\"\" \n",
    "    measure the distance for dops from the nearest main road \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    KMeans from scikitlearn \n",
    "    numpy \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clusters - number of clusters (n)\n",
    "    n_clusters_toremove - number of clusters to remove based on the distance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    minimum distance imputed possible_home df \n",
    "        \n",
    "    \"\"\"    \n",
    "    \n",
    "    cluster_df = pd.DataFrame(possible_home[['road_min_distace']])\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(cluster_df)\n",
    "    model = KMeans(n_clusters=clusters, random_state=32) \n",
    "    model.fit(X)\n",
    "    clusterlabels = model.predict(X)\n",
    "    cluster_df['cluster_name'] = clusterlabels.tolist()\n",
    "                              \n",
    "    clusters_to_keep = cluster_df.groupby('cluster_name')['road_min_distace'].max().sort_values()[n_clusters_toremove:].index\n",
    "                        \n",
    "    possible_home['cluster_labels'] = clusterlabels.tolist()\n",
    "    possible_home_selected =  possible_home[possible_home['cluster_labels'].isin(clusters_to_keep)] \n",
    "                                                        \n",
    "    return possible_home_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02ca7f",
   "metadata": {},
   "source": [
    "### 3. Filtering the trips attracted to POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb888dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "If the trip had attracted to a normal place instead of a residential place, it is assumed that a POI must stay within a radius of 15m. (a simple rule). This is because in the residential areas in general, there is less difficulty to stop nearly to a place.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4f540f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi_existance_removal(possible_home_selected, placesgpd, allowable_distance = 15):\n",
    "        \n",
    "    \"\"\" \n",
    "    remove the dops with closer pois and return the trips attracted to residential places \n",
    "    \n",
    "    dependencies \n",
    "    ----------\n",
    "    shapely \n",
    "    geopandas \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    possible_home_selected - output from the function distance_based_trip_removal\n",
    "    placesgpd - geopandas df of the POIs\n",
    "    allowable_distance - maximum allowable distance between POI and DOP to remove it \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df with records relating to trips attracted to residential places \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    indexestodrop = []\n",
    "    \n",
    "    # consider if there are any POIs exist within a radius of 15m, and if so drop it from the dataframe \n",
    "    for row in possible_home_selected.itertuples():\n",
    "        \n",
    "        DOP = [row.dropoff_lat, row.dropoff_long]\n",
    "        Point_DOP = Point(DOP[1],DOP[0]) \n",
    "        Buffer = Point_DOP.buffer(allowable_distance*0.001/110) ## (110m = 0.001) (##idk about this convertion but it is right)\n",
    "        mask = placesgpd.within(Buffer) ## filter the data for the required buffer zone \n",
    "\n",
    "        places_per_trip = placesgpd.loc[mask] ## Candidate POIs for the DOP \n",
    "\n",
    "        if  places_per_trip.shape[0] == 0:\n",
    "            possible_home_selected.loc[row.Index,'Trip purpose'] = 0 \n",
    "        \n",
    "        else:\n",
    "            indexestodrop.append(row.Index) \n",
    "    \n",
    "    actual_home_selected = possible_home_selected.drop(indexestodrop)\n",
    "                                \n",
    "    return actual_home_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "bd856549",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_home = pd.read_csv(\"results\\\\threelayer results\\\\2ndlayer\\distance_measured_level2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "141f7278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>pickup_lat</th>\n",
       "      <th>pickup_long</th>\n",
       "      <th>dropoff_lat</th>\n",
       "      <th>dropoff_long</th>\n",
       "      <th>actual_pickup_time</th>\n",
       "      <th>pickup_time</th>\n",
       "      <th>drop_time</th>\n",
       "      <th>day</th>\n",
       "      <th>cluster_no</th>\n",
       "      <th>road_min_distace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>188580986</td>\n",
       "      <td>2fd3d56199ab15009aa911c5ede7d7af7a809544e6401b...</td>\n",
       "      <td>6.85790</td>\n",
       "      <td>79.8652</td>\n",
       "      <td>6.87557</td>\n",
       "      <td>79.8716</td>\n",
       "      <td>2019-11-04 13:36:00</td>\n",
       "      <td>11/4/2019 13:30</td>\n",
       "      <td>2019-11-04 13:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.299470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>188599914</td>\n",
       "      <td>21ea654d7a43af27c92b151a005e3db936742e01930218...</td>\n",
       "      <td>6.93635</td>\n",
       "      <td>79.8441</td>\n",
       "      <td>6.88832</td>\n",
       "      <td>79.8690</td>\n",
       "      <td>2019-11-04 15:10:00</td>\n",
       "      <td>11/4/2019 15:06</td>\n",
       "      <td>2019-11-04 15:27:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.693485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>188636582</td>\n",
       "      <td>361563b79919a2016bd945a3fb99c9d1b35b5d219600c4...</td>\n",
       "      <td>6.87706</td>\n",
       "      <td>79.8584</td>\n",
       "      <td>6.88751</td>\n",
       "      <td>79.8777</td>\n",
       "      <td>2019-11-04 17:46:00</td>\n",
       "      <td>11/4/2019 17:42</td>\n",
       "      <td>2019-11-04 18:06:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.175080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>188659671</td>\n",
       "      <td>95d926da67771146e05fe6ec8e2b649580e5288914b392...</td>\n",
       "      <td>6.81778</td>\n",
       "      <td>79.9334</td>\n",
       "      <td>6.87060</td>\n",
       "      <td>79.8610</td>\n",
       "      <td>2019-11-04 19:00:00</td>\n",
       "      <td>11/4/2019 18:50</td>\n",
       "      <td>2019-11-04 20:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.600269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>188613440</td>\n",
       "      <td>8c6cc1eb8dae63629739503738a7c66b1e07a5f6520ccd...</td>\n",
       "      <td>6.94066</td>\n",
       "      <td>79.8797</td>\n",
       "      <td>6.89455</td>\n",
       "      <td>79.8721</td>\n",
       "      <td>2019-11-04 16:33:00</td>\n",
       "      <td>11/4/2019 16:25</td>\n",
       "      <td>2019-11-04 16:59:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.143120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    trip_id                                       passenger_id  \\\n",
       "0          19  188580986  2fd3d56199ab15009aa911c5ede7d7af7a809544e6401b...   \n",
       "1          29  188599914  21ea654d7a43af27c92b151a005e3db936742e01930218...   \n",
       "2          30  188636582  361563b79919a2016bd945a3fb99c9d1b35b5d219600c4...   \n",
       "3          31  188659671  95d926da67771146e05fe6ec8e2b649580e5288914b392...   \n",
       "4          35  188613440  8c6cc1eb8dae63629739503738a7c66b1e07a5f6520ccd...   \n",
       "\n",
       "   pickup_lat  pickup_long  dropoff_lat  dropoff_long   actual_pickup_time  \\\n",
       "0     6.85790      79.8652      6.87557       79.8716  2019-11-04 13:36:00   \n",
       "1     6.93635      79.8441      6.88832       79.8690  2019-11-04 15:10:00   \n",
       "2     6.87706      79.8584      6.88751       79.8777  2019-11-04 17:46:00   \n",
       "3     6.81778      79.9334      6.87060       79.8610  2019-11-04 19:00:00   \n",
       "4     6.94066      79.8797      6.89455       79.8721  2019-11-04 16:33:00   \n",
       "\n",
       "       pickup_time            drop_time  day  cluster_no  road_min_distace  \n",
       "0  11/4/2019 13:30  2019-11-04 13:50:00    0        -1.0          0.299470  \n",
       "1  11/4/2019 15:06  2019-11-04 15:27:00    0        -1.0          0.693485  \n",
       "2  11/4/2019 17:42  2019-11-04 18:06:00    0        -1.0          0.175080  \n",
       "3  11/4/2019 18:50  2019-11-04 20:48:00    0        -1.0          0.600269  \n",
       "4  11/4/2019 16:25  2019-11-04 16:59:00    0        -1.0          4.143120  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_home.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e967b741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 55s\n"
     ]
    }
   ],
   "source": [
    "## identification of trips attracted to the houses. \n",
    "\n",
    "# removal of trips that has clustered DOPs. \n",
    "possible_home = clustered_trip_removal(taxitrips_after_regular_inference, allowable_dops = 2)\n",
    "\n",
    "# removal of trips closer to the main roads \n",
    "# 7000 records =  1h 14min 43s\n",
    "possible_home = min_road_distance_measure(possible_home,colomboroads)\n",
    "possible_home_selected = distance_based_trip_removal(possible_home, clusters = 10, n_clusters_toremove = 2)\n",
    "\n",
    "# 25000, 31min 37s \n",
    "actual_home_selected = poi_existance_removal(possible_home_selected, placesgpd, allowable_distance = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044758c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_home_selected.to_csv(\"results\\\\threelayer results\\\\2ndlayer\\\\residential_selected_n10_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "2a0b401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_home_selected.to_csv(\"results\\\\threelayer results\\\\2ndlayer\\\\residential_selected_n10_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53a89b",
   "metadata": {},
   "source": [
    "## Layer 3: Baysean purpose inference  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "edcb65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the required trips for which the purpose inference is required form baysean model  \n",
    "taxitrips_to_baysean_inference = taxitrips_after_regular_inference.loc[taxitrips_after_regular_inference.index.difference(actual_home_selected.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fc0fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array to collect the results \n",
    "trip_purposes = np.array([])\n",
    "\n",
    "for row in taxitrips_to_baysean_inference.itertuples():\n",
    "    \n",
    "    # candidate poi selection \n",
    "    candidate_pois = candidate_poi_selection(placesgpd, row.dropoff_lat, row.dropoff_long, row.drop_time, time_df, walking_radius = 100)\n",
    "    \n",
    "    # baysean inference \n",
    "    Trip_purpose = baysean_inference_ln(candidate_pois, row.dropoff_lat, row.dropoff_long, row.drop_time, huanggrid)\n",
    "    \n",
    "    # collect the results for an array \n",
    "    trip_purposes = np.append(trip_purposes, Trip_purpose)\n",
    "\n",
    "# attach to the dataframe     \n",
    "taxitrips_to_baysean_inference['Trip purpose'] = trip_purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4b89a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file \n",
    "taxitrips_to_baysean_inference.to_csv('bayesln_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d13f2",
   "metadata": {},
   "source": [
    "## Merging the results of three layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94630d7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> FINAL MERGING :</b> append the dataframes with trip purposes in all three levels\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "49d18762",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_to_baysean_inference = pd.read_csv(\"D:\\MSC\\\\Data\\\\results\\\\threelayer results\\\\bayesln_results.csv\", index_col = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3e442d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_home_selected = actual_home_selected.set_index('trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "41473413",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_identified_regular_trips = purpose_identified_regular_trips.set_index('trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a3555b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_to_baysean_inference = taxitrips_to_baysean_inference.drop(actual_home_selected.index)\n",
    "taxitrips_to_baysean_inference = taxitrips_to_baysean_inference.drop(purpose_identified_regular_trips.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c10f013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_to_baysean_inference = taxitrips_to_baysean_inference[purpose_identified_regular_trips.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "a1a53540",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_home_selected = actual_home_selected[purpose_identified_regular_trips.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7416403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_with_purpose  = purpose_identified_regular_trips.append(taxitrips_to_baysean_inference).append(actual_home_selected )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9657a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_with_purpose['Trip purpose'] = taxitrips_with_purpose['Trip purpose'].replace({0:'Residential',1:'Work',2:'Educational','home':'Residential', 'Educational':'education'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a72da1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shopping        29488\n",
       "personal        22536\n",
       "Residential     12246\n",
       "dining          12033\n",
       "medical         10215\n",
       "education        6450\n",
       "Work             6032\n",
       "transit          4029\n",
       "recreational     3005\n",
       "multiple          624\n",
       "Name: Trip purpose, dtype: int64"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxitrips_with_purpose['Trip purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fd41265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxitrips_with_purpose.to_csv(\"results\\\\threelayer results\\\\three_layer_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6e9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee8befce",
   "metadata": {},
   "source": [
    "## COMTRANS Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0827cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> COMTRANS Validation :</b> Check the R2 for possible purposes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c333e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nd array n = number of entries, ds division wise % of trips for purposes \n",
    "\n",
    "# label the origin division and destination divisions for the df\n",
    "\n",
    "\n",
    "# groupby and normalized value counts \n",
    "# reset and add to a df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the comtrans results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the R2 value. \n",
    "x_values = [1,2,3]\n",
    "y_values = [1,4,9]\n",
    "\n",
    "correlation_matrix = np.corrcoef(x_values, y_values)\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "r_squared = correlation_xy**2\n",
    "\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e251c",
   "metadata": {},
   "source": [
    "## EXTRA : Walking Radius (R) selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e22de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array to collect the results \n",
    "\n",
    "nonzero_presentage = np.array([])\n",
    "\n",
    "poi_count = np.array([])\n",
    "\n",
    "for radius in [180,200]:\n",
    "    \n",
    "    for row in taxitrips_to_baysean_inference.head(10000).itertuples():\n",
    "    \n",
    "        # candidate poi selection \n",
    "        candidate_pois = candidate_poi_selection(placesgpd, row.dropoff_lat, row.dropoff_long, row.drop_time, time_df, walking_radius = radius)\n",
    "    \n",
    "        poi_count = np.append(poi_count,candidate_pois.shape[0])\n",
    "        \n",
    "    nonzero_presentage = np.append(nonzero_presentage, np.count_nonzero(poi_count)/poi_count.shape[0])  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a1399189",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_presentage_1 = nonzero_presentage\n",
    "\n",
    "# walkingdata = pd.DataFrame(columns = ['radius','percentage'])\n",
    "# walkingdata['radius'] = [10,20,30,40,60,80,100,120,140,160]\n",
    "# walkingdata['percentage'] =  nonzero_presentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f26befdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.586050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.686267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>0.796220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80</td>\n",
       "      <td>0.829117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>0.853043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>120</td>\n",
       "      <td>0.871212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>140</td>\n",
       "      <td>0.885456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>160</td>\n",
       "      <td>0.896860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius  percentage\n",
       "0      10    0.414300\n",
       "1      20    0.586050\n",
       "2      30    0.686267\n",
       "3      40    0.750100\n",
       "4      60    0.796220\n",
       "5      80    0.829117\n",
       "6     100    0.853043\n",
       "7     120    0.871212\n",
       "8     140    0.885456\n",
       "9     160    0.896860"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walkingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ddc74303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEyCAYAAADHvMbdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoZklEQVR4nO3deXxV9Z3/8dcnCQkJgYSQhCUJhCWAqIASUdzqUqt2o462Vae1tYu1U7uMbad2Or9pZzq/+bXTdloftTPWVmq30Vq32qlbVwVB2QQEKpKEJQkCSSAhC2S7n98f94LXGOCG5OTe3Pt+Ph55mHPuyeVNhLw553vO92vujoiIyFBLi3cAERFJTioYEREJhApGREQCoYIREZFAqGBERCQQKhgREQlERrwDDFRhYaGXl5fHO4aIiADr1q1rdPei/l4bcQVTXl7O2rVr4x1DREQAM9t1vNd0iUxERAKhghERkUCoYEREJBAqGBERCYQKRkREAhFowZjZVWa2zcyqzOyOfl4fb2aPmtkmM1ttZmcEmUdERIZPYAVjZunAD4CrgXnADWY2r89h/whscPf5wE3AnUHlERGR4RXkGcxioMrda9y9C3gAWNrnmHnAHwHc/RWg3MwmBphJRESGSZAFUwLURm3XRfZF2wj8DYCZLQamAaV938jMbjGztWa2tqGhIaC4IiKpxd1p7+wJ7P2DfJLf+tnXd/nMbwB3mtkG4GXgJeBNv1t3vwe4B6CyslJLcIqIxKi9s4fagx3UHjhM7YEOdh/ooO7o9sEOpk0Yw5OfvSiQXzvIgqkDyqK2S4E90Qe4+yHgZgAzM2BH5ENERGLQ1RNiT/Ph10vkYAe1BzqoPRgulAPtXW84PicznakFOZQV5HD+rAlUFI8NLFuQBbMGqDCz6UA9cD1wY/QBZpYPdETGaD4GPBcpHRERAUIhp6Gt89jZR3SJ1B08zGsthwlFXdfJSDNKxmdTNj6HK0+fRFlB+POyghzKxmdTMCaT8L/ngxdYwbh7j5ndBjwNpAPL3H2Lmd0aef1u4DTgZ2bWC2wFPhpUHhGRRNXS0R115vHGIqk7eJiuntAbjp84Louy8Tksnl5A2fjscHlEPiaNG0162vAUyMmY+8ga0qisrHTNpiwiI8mR7t43jHvURgpkd6RQWo+8ceg5L3vUm848SgtymFqQQ0l+NqNHpcfpd/JmZrbO3Sv7e23ETdcvIpKIWo90U93QzvZ9rZEzkNfHQfa3dr7h2KyMNEojZx6V5eMjRZJNaaRQ8rJHxel3MbRUMCIiA9DU1knV/ja272+jan8b1Q1tbN/Xxt5DR44dk2YwOS+bsoJs3jK7iLLI2cfRs5LC3CzSEuQyVpBUMCIifbg7ew8dYfu+tmNlUr2/jaqGtjfclZWTmc6s4lzOnzmBWRNzmVWUy6ziXMoKchiVrqkeVTAikrJ6Q07tgY43nJFU7W+luqGdtqgHEPNzRjGrKJcrT5/IzKJcKiaOZVZxLpPHjU6JM5FTpYIRkaTX2dPLzsaOSIG0sX1/K1X726hpbH/DHVoTx2UxqziXa88uYdbEscwqyqViYi4ThvHW3mSighGRpNHR1UP1/vZjBXL0Y9eBDnojD4uYQen4bCqKx3Lx7KLwZa2Jucwsyk2awfVEoYIRkRGnuaMr6mzk9SKpbz587JiMNKO8cAyzJ47lHfMnM6s4XCIzi3LJzkyc23yTmQpGRBLWke5etuw5xJY9LWzfd/TSVjuNba/f9puVkcbMolwqy8dzfVEZFRPDA+3TJozRQHucqWBEJCF094Z4dV8rm+pa2FTXzMbaFl7d10pP5NLW2NEZzCrO5dI5RcdKpKJ4LCX52RpoT1AqGBEZdqGQU9PYzqa6ZjbVtbCxrpmtew7RGRlwz8sexfzSPD4xdwbzS/M5sySPyXmjNdA+wqhgRCRQ7k7dwcOvn5nUNbO5/tCx24BzMtM5Y0oeHzxvGvPL8plfkse0CTkqkySgghGRIbW/9QibasNlsqm+hU11LcceTsxMT+O0yWO55qwS5pfmMb80n1nFuQkzOaMMLRWMiJyylo5uNtU3Hzs72VTXwmst4SlT0gxmTxzLW08rZn5pPvNL85gzaSxZGbqDK1WoYEQkJh1dPWyuP3SsSDbVNbOzqePY69MLx7B4egFnluSxoCyf06eMIydTP2JSmf7vi8ibdPWEeGXvITbWtbCpNlwo2/e3HlvYakreaOaX5vPeyjIWRAbh83L0kKK8kQpGJMX1hpyq/W1srGs+dnbyymutdPWG7+iaMCaT+aV5XHnGJBZExk2KxmbFObWMBCoYkRTTeqSbVdVNvLjjAJsid3Qd7u4FYGxWBmeW5nHzheUsiIyblORn644uOSUqGJEk19MbYmNdC8u3N7BieyMv1TbTG3KyMtI4fco43n9OGQvKwmcm0yeM0UOLMmRUMCJJaFdTO89tb2TF9gZWVjfReqQHM5hfms8n3zKTiyoKOWvqeDIzNJWKBEcFI5IEWjq6WVndyPKqRpZvb6D2QHjSx5L8bN45fzIXVRRx/swJ5OdkxjmppBIVjMgI1N0bYkNtM8tfbWB5VSMba5sJOeRmZbBk5gQ+ftEMLqooolxPxEscqWBERgB3Z0djO8u3N7J8eyMv1DTR1tlDmsHCsnxuu6yCiysKWVCWrxmEJWGoYEQS1MH2Lp6vbmRFpFSOrnUytSCHpQuncFFFIUtmFmqRLElYKhiRBNHVE2L97oPH7vbaVN+Ce3ia+gtmFvLJS8KD89MmjIl3VJGYqGBE4sTdqW5o47lXG1lRFb7s1dHVS3qacfbUfD53+Wwuml3I/JI8MnTZS0YgFYzIMGpq6+T56qbw4Pz2RvYeCk8MOb1wDNctKuXCWYUsmTmBsaN12UtGPhWMSIA6e3pZt/Ng+JmUqgY21x8CwgtqXTirkAsrCrlwViFlBTlxTioy9FQwIkPI3Xl1XxvLt4fPUF7c0cSR7hAZacbZ08bzhbfN5sKKIs4sydMaKJL0VDAig+TurN11kIfX1fGnV/azv7UTgJlFY7j+nKlcVFHIuTMmkJulv26SWvQnXuQU1R3s4JH19Ty8vo5dTR3kZKZz6dxi3lJRxIUVhUzJz453RJG4UsGIDEBHVw9PvryXh9bVsaqmCYAlMybwmcsquOqMSYzRWYrIMfrbIHISoZCzeucBHlpXx5Mvv0Z7Vy9TC3K4/YrZXHNWiQboRY5DBSNyHLubOnh4fR0Pr6+j7uBhcrMyeOf8KVxXWUrltPGa40vkJFQwIlHaOnt44uXXeGhdHat3HMAMLphZyBfeNocrT59EdmZ6vCOKjBgqGEl5oZDzQk1T+BLY5r0c7u5leuEYvnjlHK45q0SD9SKnSAUjKWtnYzsPr6/jkfX11DcfZmxWBu85q4TrFpVw9lRdAhMZLBWMpJRDR7p5YlP4EtjaXQdJM7iwoogvXT2Xt82byOhRugQmMlRUMJL0ekPOyupGHlpXx9Nb9nKkO8TMojF86aq5XHNWCZPyRsc7okhSUsFI0qpuaOPhdXU8+lI9r7UcYdzoDK5bVMq1Z5eysCxfl8BEAqaCkaTScrib/920h4fW1fHS7mbSDN4yu4ivvOM03nqaLoGJDKdAC8bMrgLuBNKBH7v7N/q8ngf8ApgayfJtd/9JkJkk+fSGnOXbG3hoXR3PbN1HV0+I2RNz+fLV4UtgxeN0CUwkHgIrGDNLB34AXAHUAWvM7HF33xp12KeAre7+LjMrAraZ2S/dvSuoXJI8tu9r5aH1dTz2Uj37DnWSnzOKG84p49pFpZxZkqdLYCJxFuQZzGKgyt1rAMzsAWApEF0wDoy18E+CXOAA0BNgJhnhmju6+O3G8CWwjXUtpKcZl84p4mvvKuWy04rJytAlMJFEEWTBlAC1Udt1wLl9jrkLeBzYA4wF3u/uob5vZGa3ALcATJ06NZCwkrh6ekM8F7kE9oet++nqDTF30lj+6R2nsXRhCUVjs+IdUUT6EWTB9Hd9wvtsXwlsAC4DZgK/N7Pl7n7oDV/kfg9wD0BlZWXf95Ak1dDayY9X1PDwunoa2zopGJPJjedO5bpFpZw+ZZwugYkkuCALpg4oi9ouJXymEu1m4Bvu7kCVme0A5gKrA8wlCa6lo5t7llezbMVOunpDXDa3mOsWlXLpnGIyM9LiHU9EYhRkwawBKsxsOlAPXA/c2OeY3cDlwHIzmwjMAWoCzCQJrKOrh588v5MfPlvNoSM9vGvBFG6/YjbTC8fEO5qInILACsbde8zsNuBpwrcpL3P3LWZ2a+T1u4GvA/eZ2cuEL6l9yd0bg8okiamzp5cHVtfy/T9V0djWyeVzi/n82+Ywb8q4eEcTkUEI9DkYd38CeKLPvrujPt8DvC3IDJK4enpDPPpSPd/7w3bqmw9z7vQCfvjBs1k0rSDe0URkCOhJfhl2oZDz1Ja9fOeZbVQ3tDO/NI9vXHsmF84q1MC9SBJRwciwcXeefbWBbz+zjc31h6gozuXuDyziytMnqlhEkpAKRobFmp0H+NZT21i98wBlBdn85/sWsHRhCelpKhaRZKWCkUBtrm/h289s4y/bGigem8XX33MG768s0+3GIilABSOBqNrfxnd//yq/e/k18nNG8eWr53LTknKtaS+SQlQwMqTqDnZw5x+28/D6OkaPSuczl83iYxfPYNzoUfGOJiLDTAUjQ6KhtZMf/LmK/3lxNxjcfMF0PnnJTApzNU+YSKpSwcig9J3W5X2VpXz6sgqm5GfHO5qIxJkKRk5J32ld3r1gCn+vaV1EJIoKRgaks6eX+1/czV1/rqaxrZO3nlbM7VdoWhcReTMVjMSkpzfEIy/Vc2dkWpfzZhTwww8uYtG08fGOJiIJSgUjJxQKOU9u3st3fr+NGk3rIiIDoIKRfrk7f3m1gW8/vY0tezSti4gMnApG3mT1jgN8+2lN6yIig6OCkWM0rYuIDCUVjGhaFxEJhAomhUVP65I9Kp3PXF7Bxy6armldRGRIqGBS0NFpXX754i7MjI9EpnWZoGldRGQIqWBSzLOvNnDrz9dFpnUp4zOXz2JynqZ1EZGhp4JJIQfau/j8gxuZWpDD3R9cpGldRCRQKpgU4e585dGXOXS4m59/dLHKRUQCp/tPU8SjL9Xz5Oa93P622Zw2WfOGiUjwVDApYE/zYb76my2cUz6ej180I95xRCRFqGCSXCjkfOHXG+l15zvvXain8UVk2KhgktxPV+1kZXUT/+ed85g6ISfecUQkhahgkljV/la+8eQrXD63mOvPKYt3HBFJMSqYJNXdG+Lvf7WRnMx0/t+1Z2oGZBEZdrpNOUnd9acqXq5v4b//9myKx46OdxwRSUE6g0lCG2qbuevPVfzNWSVcfebkeMcRkRSlgkkyh7t6uf3BDRSPzeKr7z493nFEJIXpElmS+eZTr1DT0M4vP3YuedmaFVlE4kdnMElk+fYG7lu5kw+fX84FswrjHUdEUpwKJkm0dHTzxV9vYmbRGO64em6844iIqGCSxVcf30xDWyffff9CRo/SSpQiEn8qmCTwu02v8diGPXz6slnML82PdxwREUAFM+LtP3SErzz2MgtK8/jUpbPiHUdE5BgVzAjm7vzDw5s43NXLd963kFHp+t8pIolDP5FGsPtX1/KXbQ18+eq5zCrOjXccEZE3UMGMULua2vm3323lwlmF3LSkPN5xRETeJOaCMbNsM5sTZBiJTW/Iuf3BjaSnGf9x3XzStMaLiCSgmArGzN4FbACeimwvNLPHY/i6q8xsm5lVmdkd/bz+RTPbEPnYbGa9ZlYwwN9Dyvnhc9Ws23WQry89gyn52fGOIyLSr1jPYL4GLAaaAdx9A1B+oi8ws3TgB8DVwDzgBjObF32Mu3/L3Re6+0Lgy8Cz7n4g5vQpaMueFr77+1d5+5mTWLpwSrzjiIgcV6wF0+PuLQN878VAlbvXuHsX8ACw9ATH3wDcP8BfI6V09vRy+682kp+Tyb+9R2u8iEhii7VgNpvZjUC6mVWY2feBlSf5mhKgNmq7LrLvTcwsB7gKeDjGPCnpP595lW37WvnmtWdSMCYz3nFERE4o1oL5NHA60En4LOMQ8LmTfE1//7z24xz7LuD5410eM7NbzGytma1taGiILXGSebGmiXuW13DD4qlcNndivOOIiJxUTNP1u3sH8JXIR6zqgOiF4EuBPcc59npOcHnM3e8B7gGorKw8XkklrbbOHj7/642Ujc/hn95xWrzjiIjEJKaCMbPf8uazjxZgLfBDdz/Sz5etASrMbDpQT7hEbuznvfOAtwAfGEDulPL1325lT/NhHvzEEsZkaQkfERkZYr1EVgO0AT+KfBwC9gGzI9tv4u49wG3A08BfgQfdfYuZ3Wpmt0Ydeg3wjLu3n9pvIbn9Yes+frW2lk+8ZSaV5bqDW0RGDnM/+RUnM3vO3S/ub5+ZbXH3YVubt7Ky0teuXTtcv1xcNbV1cuX3nqMwN4vf3HYBWRmahl9EEouZrXP3yv5ei/UMpsjMpka94VTg6JKJXYPMJ8fxlUc3c+hwD9+7fqHKRURGnFgv6H8eWGFm1YTvDpsO/J2ZjQF+GlS4VLahtpmntuzli1fOYe6kcfGOIyIyYLHeRfaEmVUAcwkXzCtRA/vfCyhbSlu2YgdjszL40Pnl8Y4iInJKBnJLUgUwBxgNzDcz3P1nwcRKba+1HOaJl1/jw+eXk6u7xkRkhIr1NuWvApcQnlPsCcLzi60AVDAB+NmqXYTcdfYiIiNarIP81wGXA3vd/WZgAZAVWKoU1tHVw/+8uJsrT59EWUFOvOOIiJyyWAvmsLuHgB4zGwfsB2YEFyt1PbK+npbD3XzkwunxjiIiMiixXuBfa2b5hB+qXEf4ocvVQYVKVaGQs+z5HcwvzaNy2vh4xxERGZRY7yL7u8ind5vZU8A4d98UXKzU9Oz2Bmoa2vne+xdqKn4RGfFiXdHyj0c/d/ed7r4pep8MjWUrdjBxXBZvP3NyvKOIiAzaCc9gzGw0kAMUmtl4Xp+Cfxyg5RSH0Kv7Wlm+vZEvXjmHzIxYh8ZERBLXyS6RfYLwui9TCI+9HC2YQ4SXQ5YhsmzFDrIy0rhx8dSTHywiMgKcsGDc/U7gTjP7tLt/f5gypZymtk4eeamea88uZbxWqhSRJBHrIP/3zex8oDz6a/Qk/9C4f/VuunpCfOSC8nhHEREZMrE+yf9zYCawAeiN7Hb0JP+gdfWE+NmqXVw8u4iKiWPjHUdEZMjE+hxMJTDPY1k8Rgbkdy/vYX9rJ996rx6sFJHkEuvtSpuBSUEGSUXuzr0rdjCrOJeLKwpP/gUiIiNIrGcwhcBWM1sNdB7d6e7vDiRViliz8yCb6w/x79ecqQcrRSTpxFowXwsyRKq6d0UN+TmjuOasknhHEREZcrHeRfasmU0DKtz9D2aWA2gN30HY3dTBM1v38cm3zCQ7U99KEUk+sU4V83HgIeCHkV0lwGMBZUoJ963cSboZNy0pj3cUEZFAxDrI/yngAsJP8OPu24HioEIlu9Yj3Ty4tpZ3zJ/MpLzR8Y4jIhKIWAum0927jm6YWQbh52DkFDy4to62zh4+qjVfRCSJxVowz5rZPwLZZnYF8Gvgt8HFSl69Iee+lTuonDae+aX58Y4jIhKYWAvmDqABeJnwBJhPAP8UVKhk9vut+6g9cFhnLyKS9GK9TTkbWObuPwIws/TIvo6ggiWrZc/voCQ/myvmTYx3FBGRQMV6BvNHwoVyVDbwh6GPk9w217ewescBbr6gnIx0rfkiIskt1p9yo9297ehG5POcYCIlr2UrdjAmM533nVMW7ygiIoGLtWDazezsoxtmtgg4HEyk5LT/0BF+u2kP760sY9zoUfGOIyISuFjHYD4L/NrM9kS2JwPvDyZScvr5C7voCTk3a80XEUkRJy2YyID+RcBcYA7hZZNfcffugLMljSPdvfzyxd289bSJTJswJt5xRESGxUkvkbl7L7DU3bvdfbO7v6xyGZjHXqrnQHsXH7lAtyaLSOqI9RLZ82Z2F/AroP3oTndfH0iqJOLuLHt+B6dNHsd5MwriHUdEZNjEWjDnR/77r1H7HLhsaOMknxVVjby6r41vv3eB1nwRkZQS63T9lwYdJFndu2IHhblZvGvB5HhHEREZVrFO1z/RzO41sycj2/PM7KPBRhv5qva38ZdtDXzwvGlkZWjNFxFJLbE+B3Mf8DQwJbL9KvC5APIklZ88v4PMjDT+9ryp8Y4iIjLsYi2YQnd/EAgBuHsP0BtYqiTQ3NHFw+vreM/CKRTmZsU7jojIsBvIk/wTiKwBY2bnAS2BpUoC96+u5Uh3iI9o1mQRSVGx3kV2O/A4MMPMngeKgOsCSzXCdfeG+OnKnVwwawJzJ42LdxwRkbiI9QxmK/AosAbYB/yI8DjMCZnZVWa2zcyqzOyO4xxziZltMLMtZvZsrMET2ZOb97L30BE9WCkiKS3WM5ifAYeAf49s3wD8HHjv8b4gMsXMD4ArgDpgjZk97u5bo47JB/4LuMrdd5tZ8YB/BwnG3bl3xQ6mF47h0jkj/rcjInLKYi2YOe6+IGr7z2a28SRfsxiocvcaADN7AFhK+GzoqBuBR9x9N4C7748xT8Jav7uZjbXN/OvS00lL04OVIpK6Yr1E9lJkYB8AMzsXeP4kX1MC1EZt10X2RZsNjDezv5jZOjO7KcY8CWvZih2MG53BtWeXxjuKiEhcxXoGcy5wk5ntjmxPBf5qZi8D7u7z+/ma/v757v38+ouAywmvkrnKzF5w9zeM75jZLcAtAFOnJu4zJXUHO3hy82t8/KIZjMmK9VsrIpKcYv0peNUpvHcdEL10Yymwp59jGt29nfCt0M8BC+hzA4G73wPcA1BZWdm3pBLGz1btwsy46fzyeEcREYm7WOci23UK770GqDCz6UA9cD3hMZdovwHuMrMMIJPwmdJ3T+HXirv2zh7uX72bq86YREl+drzjiIjEXWDXcdy9x8xuIzzFTDqwzN23mNmtkdfvdve/mtlTwCbCswT82N03B5UpSA+tq6P1SI9uTRYRiQh0oMDdnwCe6LPv7j7b3wK+FWSOoIVCzk+e38HCsnwWTRsf7zgiIgkh1rvI5AT+9Mp+djZ1aFoYEZEoKpgh8Ku1tUwcl8XVZ0yKdxQRkYShghmknt4QL1Q3cdnciYxK17dTROQo/UQcpC17DtHa2cOSmRPiHUVEJKGoYAZpZXUTAEtmqGBERKKpYAZpZXUjsyfmUjRWi4qJiERTwQxCV0+ItTsP6uxFRKQfKphB2FjXzOHuXpbMLIx3FBGRhKOCGYRV1U2YwXkzCuIdRUQk4ahgBmFldSPzJo8jPycz3lFERBKOCuYUHenuZf2uZs7X7ckiIv1SwZyi9bsO0tUb4nyNv4iI9EsFc4pWVjeRnmacM13jLyIi/VHBnKKV1Y3ML80jVytXioj0SwVzCto6e9hU16LnX0RETkAFcwrW7DxAT8g1/iIicgIqmFPwQnUTmelpWlxMROQEVDCnYGV1Ewun5pOdmR7vKCIiCUsFM0AtHd1s3tOi519ERE5CBTNAL+5owh2Nv4iInIQKZoBWVjcxelQaC8ry4h1FRCShqWAGaFV1E+eUF5CVofEXEZETUcEMQGNbJ9v2tWp5ZBGRGKhgBuCFGi2PLCISKxXMAKysbiI3K4MzSzT+IiJyMiqYAXihuonF0wvISNe3TUTkZPSTMkZ7W45Q09iu519ERGKkgonRqppGAA3wi4jESAUTo5VVTeTnjOK0SePiHUVEZERQwcRoZXUT502fQFqaxTuKiMiIoIKJQe2BDuqbD3P+LF0eExGJlQomBiurI+Mvev5FRCRmKpgYrKxuojA3i1nFufGOIiIyYqhgTsLdWVXdxPkzJ2Cm8RcRkVipYE6iuqGd/a2duj1ZRGSAVDAnsSoy/qIHLEVEBkYFcxKrapqYkjeaqQU58Y4iIjKiqGBOIBQKj78smVmo8RcRkQFSwZzAtn2tHOzo1uUxEZFToII5gZXVkfVfVDAiIgMWaMGY2VVmts3Mqszsjn5ev8TMWsxsQ+Tjn4PMM1Crqhspn5DDlPzseEcRERlxMoJ6YzNLB34AXAHUAWvM7HF339rn0OXu/s6gcpyqnt4QL9Yc4J0LpsQ7iojIiBTkGcxioMrda9y9C3gAWBrgrzektuw5RGtnjy6PiYicoiALpgSojdqui+zra4mZbTSzJ83s9ADzDMix8RfNPyYickoCu0QG9Hdfr/fZXg9Mc/c2M3s78BhQ8aY3MrsFuAVg6tSpQxyzf6tqmpg9MZeisVnD8uuJiCSbIM9g6oCyqO1SYE/0Ae5+yN3bIp8/AYwys8K+b+Tu97h7pbtXFhUVBRg5rKsnxJodB3T2IiIyCEEWzBqgwsymm1kmcD3wePQBZjbJIk8wmtniSJ6mADPFZGNdM4e7e1ky801dJyIiMQrsEpm795jZbcDTQDqwzN23mNmtkdfvBq4DPmlmPcBh4Hp373sZbditqm7CDM6bURDvKCIiI1aQYzBHL3s90Wff3VGf3wXcFWSGU7GyupF5k8eRn5MZ7ygiIiOWnuTv40h3L+t3N2t6GBGRQVLB9LF+10G6ekJ6/kVEZJBUMH2srG4iPc04p1zjLyIig6GC6WNVTRPzS/MYO3pUvKOIiIxoKpgobZ09bKxt1vMvIiJDQAUTZc3OA/SEnPP1/IuIyKCpYKK8UN1EZnoai6aNj3cUEZERTwUTZWV1Ewun5pOdmR7vKCIiI54KJqKlo5vNe1r0/IuIyBBRwUS8uKMJd03PLyIyVFQwESurmxg9Ko2FU/PjHUVEJCmoYCJeqGninPICsjI0/iIiMhRUMEBjWyev7G3lPF0eExEZMioYwmcvgAb4RUSGkAqG8PovuVkZnFmSF+8oIiJJQwVDuGAWTy8gI13fDhGRoZLyP1H3thyhprFdl8dERIZYyhfMqppGAK3/IiIyxFK+YFZWNZGfM4rTJo2LdxQRkaSigqlu4rzpE0hLs3hHERFJKildMLUHOqhvPqzLYyIiAUjpgllZHR5/0QC/iMjQS+mCWVXdRGFuFrOKc+MdRUQk6aRswbg7K6ubWDJzAmYafxERGWopWzDVDe3sb+3U5TERkYCkbMGs0vxjIiKBSt2CqW5kSt5ophbkxDuKiEhSSsmCCYWcVdVNLJlZqPEXEZGApGTBbNvXysGObl0eExEJUEoWzMrq8PiLHrAUEQlOShbMqupGyifkMCU/O95RRESSVsoVTE9viBdrDujsRUQkYClXMFv2HKK1s4clMwvjHUVEJKmlXMEcff5lyQydwYiIBCnlCmZldRMVxbkUjc2KdxQRkaSWUgXT1RNizY4Duj1ZRGQYpFTBbKpr5nB3r8ZfRESGQUoVjAMXzirkvBkF8Y4iIpL0MuIdYDidU17ALz52brxjiIikhJQ6gxERkeETaMGY2VVmts3MqszsjhMcd46Z9ZrZdUHmERGR4RNYwZhZOvAD4GpgHnCDmc07znHfBJ4OKouIiAy/IM9gFgNV7l7j7l3AA8DSfo77NPAwsD/ALCIiMsyCLJgSoDZquy6y7xgzKwGuAe4OMIeIiMRBkAXT30pe3mf7e8CX3L33hG9kdouZrTWztQ0NDUOVT0REAhTkbcp1QFnUdimwp88xlcADkVUlC4G3m1mPuz8WfZC73wPcA1BZWdm3pEREJAEFWTBrgAozmw7UA9cDN0Yf4O7Tj35uZvcB/9u3XEREZGQKrGDcvcfMbiN8d1g6sMzdt5jZrZHXNe4iIpLEAn2S392fAJ7os6/fYnH3DweZRUREhpe5j6whDTNrAHbFO0c/CoHGeIeI0UjKCiMrr7IGZyTlTaWs09y9qL8XRlzBJCozW+vulfHOEYuRlBVGVl5lDc5IyqusYZqLTEREAqGCERGRQKhghs498Q4wACMpK4ysvMoanJGUV1nRGIyIiAREZzAiIhIIFcwAmVmZmf3ZzP5qZlvM7LOR/QVm9nsz2x757/h4Zz3KzNLN7CUz+9/IdiJnzTezh8zslcj3eEmi5jWzv4/8GdhsZveb2ehEympmy8xsv5ltjtp33Hxm9uXI2k3bzOzKBMj6rcifg01m9qiZ5Sdq1qjXvmBmbmaFiZA18uv3m9fMPh3JtMXM/iOIvCqYgesBPu/upwHnAZ+KrHNzB/BHd68A/hjZThSfBf4atZ3IWe8EnnL3ucACwrkTLm9kJvDPAJXufgbh2SquJ7Gy3gdc1Wdfv/kif4avB06PfM1/RdZqGi738easvwfOcPf5wKvAlyFhs2JmZcAVwO6offHOCv3kNbNLCS+fMt/dTwe+Hdk/pHlVMAPk7q+5+/rI562EfwCWEP6f9dPIYT8F3hOXgH2YWSnwDuDHUbsTNes44GLgXgB373L3ZhI0L+GZMLLNLAPIITyZa8JkdffngAN9dh8v31LgAXfvdPcdQBXhNZ2GRX9Z3f0Zd++JbL5AeMLchMwa8V3gH3jjrPFxzQrHzftJ4Bvu3hk55uh6XEOaVwUzCGZWDpwFvAhMdPfXIFxCQHEco0X7HuE/9KGofYmadQbQAPwkcknvx2Y2hgTM6+71hP/Vtxt4DWhx92dIwKx9HC/fSddvirOPAE9GPk+4rGb2bqDe3Tf2eSnhskbMBi4ysxfN7FkzOyeyf0jzqmBOkZnlEl6J83PufijeefpjZu8E9rv7unhniVEGcDbw3+5+FtBOAlwO609k7GIpMB2YAowxsw/EN9WgxLJ+U1yY2VcIX5r+5dFd/RwWt6xmlgN8Bfjn/l7uZ18ifF8zgPGEL/N/EXjQzIwhzquCOQVmNopwufzS3R+J7N5nZpMjr08mMZaAvgB4t5ntJLxk9WVm9gsSMyuE/7VU5+4vRrYfIlw4iZj3rcAOd29w927gEeB8EjNrtOPli2X9pmFnZh8C3gn8rb/+TEWiZZ1J+B8aGyN/10qB9WY2icTLelQd8IiHrSZ8haOQIc6rghmgSMvfC/zV3f8z6qXHgQ9FPv8Q8JvhztaXu3/Z3UvdvZzwwN2f3P0DJGBWAHffC9Sa2ZzIrsuBrSRm3t3AeWaWE/kzcTnh8bhEzBrtePkeB643sywLr+FUAayOQ75jzOwq4EvAu929I+qlhMrq7i+7e7G7l0f+rtUBZ0f+PCdU1iiPAZcBmNlsIJPwhJdDm9fd9TGAD+BCwqeMm4ANkY+3AxMI35WzPfLfgnhn7ZP7EsILupHIWYGFwNrI9/cxwqfxCZkX+BfgFWAz8HMgK5GyAvcTHh/qJvxD76Mnykf4Mk81sA24OgGyVhEeDzj69+zuRM3a5/WdQGEiZD3B9zYT+EXkz+564LIg8upJfhERCYQukYmISCBUMCIiEggVjIiIBEIFIyIigVDBiIhIIFQwIsPMzC6x12e2freZJeRsBSKDlRHvACLJ4uhUG+4eOunBEe7+OOGH20SSjs5gRAbBzMotvG7NfxF+YO1eM1sbWWPjX6KOuyqytskK4G+i9n/YzO6KfH6fmV0X9Vpb5L+Tzew5M9tg4bVnLhq236DIIOgMRmTw5gA3u/vfmVmBux+IrKHxRzM7upbJjwhPzVEF/GqA738j8LS7/9/I++YMZXiRoOgMRmTwdrn7C5HP32dm64GXCC/aNA+YS3hizO0enjrjFwN8/zXAzWb2NeBMD69DJJLwVDAig9cOEJkc8AvA5R5ehfF3wOjIMbHMydRD5O9kZDwnE44tGHUxUA/83MxuGtL0IgFRwYgMnXGEy6bFzCYCV0f2vwJMN7OZke0bjvP1O4FFkc+XAqMAzGwa4XV9fkR4Ju+zhz66yNDTGIzIEHH3jWb2ErAFqAGej+w/Yma3AL8zs0ZgBXBGP2/xI+A3Zraa8EzH7ZH9lwBfNLNuoA3QGYyMCJpNWUREAqFLZCIiEggVjIiIBEIFIyIigVDBiIhIIFQwIiISCBWMiIgEQgUjIiKBUMGIiEgg/j+XMYUIPPYWpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 460.8x345.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6.4,4.8))\n",
    "ax = sns.lineplot(data=walkingdata, x=\"radius\", y=\"percentage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
